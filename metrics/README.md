This directory contains implementations of various evaluation metrics. All metrics inherit from the `metric.py` base class. The '--metrics' parameter controls which metrics are run in training and validation phases, simply include the the name of the metric python file without '.py'. By default a metric class implements a function `update` that takes two arguments, prediction and target, that are the outputs of a `criterion` class. The `update` function is called in every iteration, and at the end of each epoch the `compute` function is called. `compute` returns a tuple that consists of the name of the metric (should be the same as the filename) and the score, which will be stored in the experiment file for that epoch. Note that a metric computed in the validation phase will have it's name prefixed by 'val_' and a metric computed in the training phase will have it's name prefixed by 'train_'. Furthermore, a metric computed in a `task` class with have it's name prefixed by the name of that task file, followed by an underscore. Please keep this in mind when choosing a '--metric' (for computing best model).
